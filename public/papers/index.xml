<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Papers on David Paulius, Ph.D. | Personal Website</title>
    <link>https://davidpaulius.github.io/papers/</link>
    <description>Recent content in Papers on David Paulius, Ph.D. | Personal Website</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <copyright>2025 David Paulius</copyright>
    <lastBuildDate>Sun, 15 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://davidpaulius.github.io/papers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Least Commitment Planning for the Object Scouting Problem</title>
      <link>https://davidpaulius.github.io/papers/iros25/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/iros25/</guid>
      <description>TL;DR &amp;ndash; This paper introduces a new planning framework specifically designed for object scouting with LOMDPs called the Scouting Partial-Order Planner (SPOP), which exploits the characteristics of partial order and regression planning to plan around gaps in knowledge the robot may have about the existence, location, and state of relevant objects in its environment.</description>
    </item>
    <item>
      <title>Bootstrapping Object-level Planning with Large Language Models</title>
      <link>https://davidpaulius.github.io/papers/olp-icra25/</link>
      <pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/olp-icra25/</guid>
      <description>TL;DR &amp;ndash; This paper formalizes the concept of object-level planning and discusses how this level of planning naturally integrates with large language models (LLMs).</description>
    </item>
    <item>
      <title>Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models</title>
      <link>https://davidpaulius.github.io/papers/IROS24_spatiotemp/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/IROS24_spatiotemp/</guid>
      <description>TL;DR &amp;ndash; Building on prior work (Lang2LTL - CoRL 2023), this paper introduces a modular system that enables robots to follow natural language commands with spatiotemporal referring expressions. This system leverages multi-modal foundation models as well as the formal language LTL (linear temporal logic).</description>
    </item>
    <item>
      <title>CAPE: Corrective Actions from Precondition Errors using Large Language Models</title>
      <link>https://davidpaulius.github.io/papers/cape/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/cape/</guid>
      <description>TL;DR &amp;ndash; In this paper, we introduce CAPE: an approach to correct errors encountered during robot plan execution. We exploit the ability of large language models to generate high-level plans and to reason about causes of errors.</description>
    </item>
    <item>
      <title>Skill Generalization With Verbs</title>
      <link>https://davidpaulius.github.io/papers/IROS23_skill_gen/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/IROS23_skill_gen/</guid>
      <description>TL;DR &amp;ndash; This paper introduces a deep learning-based method for learning about the effects of verbs &amp;ndash; more specifically, looking at initiation and termination conditions as with Markov Decision Processes (MDPs).</description>
    </item>
    <item>
      <title>Long-Horizon Planning and Execution with Functional Object-Oriented Networks</title>
      <link>https://davidpaulius.github.io/papers/foon_lhpe/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/foon_lhpe/</guid>
      <description>TL;DR &amp;ndash; In this paper, we introduce the idea of connecting FOONs to robotic task and motion planning. We automatically transform a FOON graph, which exists at the object level (i.e., it is a representation that uses meaningful labels or expressions close to human language), into task planning specifications written in PDDL (not a very intuitive way to communicate about tasks).</description>
    </item>
    <item>
      <title>Object-Level Planning and Abstraction</title>
      <link>https://davidpaulius.github.io/papers/olp_bluesky/</link>
      <pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/olp_bluesky/</guid>
      <description>TL;DR &amp;ndash; This workshop paper (specifically, a blue-sky submission) introduces the importance of object-level planning and representation as an additional layer on top of task and motion planning. I present several benefits of using object-level planning for long-term use in robotics.</description>
    </item>
    <item>
      <title>Approximate Task Tree Retrieval in a Knowledge Network for Robotic Cooking</title>
      <link>https://davidpaulius.github.io/papers/foon_tree_retrieval/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/foon_tree_retrieval/</guid>
      <description>TL;DR &amp;ndash; In this paper, we introduce the idea of connecting FOONs to robotic task and motion planning. We automatically transform a FOON graph, which exists at the object level (i.e., it is a representation that uses meaningful labels or expressions close to human language), into task planning specifications written in PDDL (not a very intuitive way to communicate about tasks).</description>
    </item>
    <item>
      <title>Robot Learning of Assembly Tasks from Non-expert Demonstrations using Functional Object-Oriented Network</title>
      <link>https://davidpaulius.github.io/papers/foon_assembly/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/foon_assembly/</guid>
      <description>TL;DR &amp;ndash; This was a collaboration with Clemson University&amp;rsquo;s Yunyi Jia and Yi Chen, who were interested in using FOONs for representing assembly tasks. They successfully utilized and adapted a FOON to robotic assembly execution.</description>
    </item>
    <item>
      <title>Task Planning with a Weighted Functional Object-Oriented Network</title>
      <link>https://davidpaulius.github.io/papers/foon_cobot/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/foon_cobot/</guid>
      <description>TL;DR &amp;ndash; In this paper, we attempt to execute task plan sequences extracted from FOONs. However, these sequences may contain actions that are not executable by a robot. Therefore, a human is introduced in the planning and execution loop, and both the robot and human assistant work together to solve the task.</description>
    </item>
    <item>
      <title>Developing Motion Code Embedding for Action Recognition in Videos</title>
      <link>https://davidpaulius.github.io/papers/motion_codes_ICPR20/</link>
      <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/motion_codes_ICPR20/</guid>
      <description>TL;DR &amp;ndash; This work uses the features from the motion taxonomy to improve action recognition on egocentric videos from the EPIC-KITCHENS dataset. This is done by integrating motion code detection for action sequences.</description>
    </item>
    <item>
      <title>Estimating Motion Codes from Demonstration Videos</title>
      <link>https://davidpaulius.github.io/papers/motion_codes_IROS20/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/motion_codes_IROS20/</guid>
      <description>TL;DR &amp;ndash; In this work, we showed how motion codes (which can be constructed using the motion taxonomy proposed in our RSS 2020 paper) can be used to improve action recognition with deep neural networks.</description>
    </item>
    <item>
      <title>A Motion Taxonomy for Manipulation Embedding</title>
      <link>https://davidpaulius.github.io/papers/motion_codes_RSS20/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/motion_codes_RSS20/</guid>
      <description>TL;DR &amp;ndash; In this work, we introduce new changes to the features of the motion taxonomy and show how action verbs encoded as motion codes better capture differences between them than conventional word embedding (as word2vec).</description>
    </item>
    <item>
      <title>Manipulation Motion Taxonomy and Coding for Robots</title>
      <link>https://davidpaulius.github.io/papers/motion_codes_IROS19/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/motion_codes_IROS19/</guid>
      <description>TL;DR &amp;ndash; This paper introduces the motion taxonomy, a collection of robot-relevant features that are better suited for verb or action embedding than conventional word embedding. Motion codes are constructed per verb using the taxonomy. In this work, we show that motion codes assigned to verbs are closely related to one another based on force and trajectory data.</description>
    </item>
    <item>
      <title>A Survey of Knowledge Representation in Service Robotics</title>
      <link>https://davidpaulius.github.io/papers/survey_kr/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/survey_kr/</guid>
      <description>TL;DR &amp;ndash; This was my first survey paper that covers knowledge representations for service robotics. Although it is dated, it covers an extensive list of approaches used to represent knowledge for several robot sub-tasks.</description>
    </item>
    <item>
      <title>Long Activity Video Understanding using Functional Object-Oriented Network</title>
      <link>https://davidpaulius.github.io/papers/foon_video_understanding/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/foon_video_understanding/</guid>
      <description>TL;DR &amp;ndash; This work leverages functional object-oriented networks and deep learning for video understanding. In addition, with the deep network framework, we jointly recognize object and action types, which can then be used for constructing new FOON structures.</description>
    </item>
    <item>
      <title>Functional Object-Oriented Network: Construction &amp; Expansion</title>
      <link>https://davidpaulius.github.io/papers/foon_gen_exp/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/foon_gen_exp/</guid>
      <description>TL;DR &amp;ndash; In this paper, we explore methods in natural language processing (NLP) &amp;ndash; specifically semantic similarity &amp;ndash; for expanding or generalizing knowledge contained in a FOON. This alleviates the need for demonstrating and annotating graphs by other means.</description>
    </item>
    <item>
      <title>Functional Object-Oriented Network for Manipulation Learning</title>
      <link>https://davidpaulius.github.io/papers/foon_intro/</link>
      <pubDate>Sat, 15 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://davidpaulius.github.io/papers/foon_intro/</guid>
      <description>TL;DR &amp;ndash; This was the very first paper on FOON: the functional object-oriented network. Here, we introduced what they are and how they can be used for task planning. They are advantageous for their flexibility and human interpretability.</description>
    </item>
  </channel>
</rss>
