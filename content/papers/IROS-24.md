---
title: "Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models"
date: 2024-09-01
url: "/papers/IROS24_spatiotemp/"
tags: ["linear temporal logic", "LTL", "spatiotemporal grounding", "grounding", "natural language", "foundation models", "vision-language models", "large language models", "word embedding"]
author: ["Jason Xinyu Liu", "Ankit Shah", "George Konidaris", "Stefanie Tellex", "David Paulius"]
description: ""
summary: "TL;DR -- Building on prior work (Lang2LTL - CoRL 2023), this paper introduces a modular system that enables robots to follow natural language commands with spatiotemporal referring expressions. This system leverages multi-modal foundation models as well as the formal language LTL (linear temporal logic)."
---

##### Related Links:

+ [Website](https://spatiotemporal-ground.github.io/)
+ [Lang2LTL ver. 1](https://lang2ltl.github.io/) (CoRL 2023)
---

#### Citation:

J. X. Liu, A. Shah, G. Konidaris, S. Tellex, and D. Paulius (2024). "Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models". In: *2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.